{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9bd97fa",
   "metadata": {},
   "source": [
    "# Advanced Sleep Pattern Prediction Model Training\n",
    "This notebook trains a sophisticated ML model using your sleep data and exports it for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install supabase tensorflow scikit-learn pandas numpy matplotlib seaborn joblib python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2077e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "import json\n",
    "from supabase import create_client\n",
    "import os\n",
    "from google.colab import files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Replace with your actual Supabase credentials\n",
    "SUPABASE_URL = \"YOUR_SUPABASE_URL_HERE\"\n",
    "SUPABASE_KEY = \"YOUR_SUPABASE_ANON_KEY_HERE\"\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sleep data from Supabase\n",
    "def load_sleep_data():\n",
    "    try:\n",
    "        response = supabase.table('sleep_records').select('*').order('start_time').execute()\n",
    "        data = response.data\n",
    "        \n",
    "        if not data:\n",
    "            raise ValueError(\"No sleep data found\")\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"üìä Loaded {len(df)} sleep records\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "sleep_df = load_sleep_data()\n",
    "if sleep_df is not None:\n",
    "    print(\"\\nüìã Data Info:\")\n",
    "    print(sleep_df.info())\n",
    "    print(\"\\nüîç First 5 records:\")\n",
    "    print(sleep_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering\n",
    "def create_advanced_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    \n",
    "    # Calculate duration if not present\n",
    "    if 'sleep_duration' not in df.columns or df['sleep_duration'].isna().any():\n",
    "        df['sleep_duration'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Extract time features\n",
    "    df['start_hour'] = df['start_time'].dt.hour\n",
    "    df['start_minute'] = df['start_time'].dt.minute\n",
    "    df['day_of_week'] = df['start_time'].dt.dayofweek\n",
    "    df['month'] = df['start_time'].dt.month\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical encoding for time features\n",
    "    df['start_hour_sin'] = np.sin(2 * np.pi * df['start_hour'] / 24)\n",
    "    df['start_hour_cos'] = np.cos(2 * np.pi * df['start_hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Rolling statistics (7-day and 30-day windows)\n",
    "    df = df.sort_values('start_time')\n",
    "    df['duration_7d_avg'] = df['sleep_duration'].rolling(window=7, min_periods=1).mean()\n",
    "    df['duration_7d_std'] = df['sleep_duration'].rolling(window=7, min_periods=1).std().fillna(0)\n",
    "    df['start_hour_7d_avg'] = df['start_hour'].rolling(window=7, min_periods=1).mean()\n",
    "    \n",
    "    # Sleep debt calculation\n",
    "    target_sleep = 8.0  # hours\n",
    "    df['daily_sleep_debt'] = target_sleep - df['sleep_duration']\n",
    "    df['cumulative_sleep_debt'] = df['daily_sleep_debt'].rolling(window=7, min_periods=1).sum()\n",
    "    \n",
    "    # Sleep quality features (if available)\n",
    "    quality_cols = ['sleep_quality', 'stress_level', 'caffeine_intake', 'exercise_hours']\n",
    "    for col in quality_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_7d_avg'] = df[col].rolling(window=7, min_periods=1).mean()\n",
    "    \n",
    "    # Previous day features\n",
    "    df['prev_duration'] = df['sleep_duration'].shift(1)\n",
    "    df['prev_start_hour'] = df['start_hour'].shift(1)\n",
    "    \n",
    "    # Remove rows with NaN values for modeling\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "featured_df = create_advanced_features(sleep_df)\n",
    "print(f\"üîß Created {featured_df.shape[1]} features from {featured_df.shape[0]} records\")\n",
    "print(\"\\nüìä Feature columns:\")\n",
    "print(list(featured_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Sleep patterns over time\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(featured_df['start_time'], featured_df['start_hour'])\n",
    "plt.title('Sleep Start Time Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(featured_df['start_time'], featured_df['sleep_duration'])\n",
    "plt.title('Sleep Duration Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Day of week patterns\n",
    "plt.subplot(2, 3, 3)\n",
    "day_avg = featured_df.groupby('day_of_week')['sleep_duration'].mean()\n",
    "plt.bar(range(7), day_avg.values)\n",
    "plt.title('Average Sleep Duration by Day')\n",
    "plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "start_avg = featured_df.groupby('day_of_week')['start_hour'].mean()\n",
    "plt.bar(range(7), start_avg.values)\n",
    "plt.title('Average Sleep Start Time by Day')\n",
    "plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "# Sleep debt\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(featured_df['start_time'], featured_df['cumulative_sleep_debt'])\n",
    "plt.title('Cumulative Sleep Debt')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(2, 3, 6)\n",
    "numeric_cols = featured_df.select_dtypes(include=[np.number]).columns[:10]\n",
    "corr = featured_df[numeric_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_modeling_data(df):\n",
    "    # Select features for modeling\n",
    "    feature_cols = [\n",
    "        'start_hour_sin', 'start_hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos',\n",
    "        'is_weekend', 'duration_7d_avg', 'duration_7d_std', 'start_hour_7d_avg',\n",
    "        'cumulative_sleep_debt', 'prev_duration', 'prev_start_hour'\n",
    "    ]\n",
    "    \n",
    "    # Add quality features if available\n",
    "    quality_features = [col for col in df.columns if col.endswith('_7d_avg') and 'duration' not in col and 'start_hour' not in col]\n",
    "    feature_cols.extend(quality_features)\n",
    "    \n",
    "    # Filter existing columns\n",
    "    feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    \n",
    "    # Targets: start_hour and duration\n",
    "    y_start = df['start_hour'].values\n",
    "    y_duration = df['sleep_duration'].values\n",
    "    \n",
    "    return X, y_start, y_duration, feature_cols\n",
    "\n",
    "X, y_start, y_duration, feature_names = prepare_modeling_data(featured_df)\n",
    "print(f\"üìê Features shape: {X.shape}\")\n",
    "print(f\"üéØ Targets: start_time({len(y_start)}), duration({len(y_duration)})\")\n",
    "print(f\"üîß Using features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74310b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models and compare performance\n",
    "def train_and_evaluate_models(X, y_start, y_duration):\n",
    "    # Split data\n",
    "    X_train, X_test, y_start_train, y_start_test, y_duration_train, y_duration_test = train_test_split(\n",
    "        X, y_start, y_duration, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîÑ Training {name}...\")\n",
    "        \n",
    "        # Train for start time prediction\n",
    "        if name == 'Neural Network':\n",
    "            model_start = model\n",
    "            model_start.fit(X_train_scaled, y_start_train)\n",
    "            y_start_pred = model_start.predict(X_test_scaled)\n",
    "            \n",
    "            model_duration = MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000)\n",
    "            model_duration.fit(X_train_scaled, y_duration_train)\n",
    "            y_duration_pred = model_duration.predict(X_test_scaled)\n",
    "        else:\n",
    "            model_start = model\n",
    "            model_start.fit(X_train, y_start_train)\n",
    "            y_start_pred = model_start.predict(X_test)\n",
    "            \n",
    "            # Clone model for duration\n",
    "            if name == 'Random Forest':\n",
    "                model_duration = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            else:\n",
    "                model_duration = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "            \n",
    "            model_duration.fit(X_train, y_duration_train)\n",
    "            y_duration_pred = model_duration.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        start_mae = mean_absolute_error(y_start_test, y_start_pred)\n",
    "        start_r2 = r2_score(y_start_test, y_start_pred)\n",
    "        duration_mae = mean_absolute_error(y_duration_test, y_duration_pred)\n",
    "        duration_r2 = r2_score(y_duration_test, y_duration_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'start_mae': start_mae,\n",
    "            'start_r2': start_r2,\n",
    "            'duration_mae': duration_mae,\n",
    "            'duration_r2': duration_r2\n",
    "        }\n",
    "        \n",
    "        trained_models[name] = {\n",
    "            'start_model': model_start,\n",
    "            'duration_model': model_duration,\n",
    "            'scaler': scaler if name == 'Neural Network' else None\n",
    "        }\n",
    "        \n",
    "        print(f\"  Start Time - MAE: {start_mae:.2f}h, R¬≤: {start_r2:.3f}\")\n",
    "        print(f\"  Duration - MAE: {duration_mae:.2f}h, R¬≤: {duration_r2:.3f}\")\n",
    "    \n",
    "    return results, trained_models, scaler\n",
    "\n",
    "# Train models\n",
    "results, trained_models, scaler = train_and_evaluate_models(X, y_start, y_duration)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264709b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Deep Learning Model\n",
    "def create_deep_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(2)  # Output: [start_time, duration]\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare data for deep learning\n",
    "X_train, X_test, y_start_train, y_start_test, y_duration_train, y_duration_test = train_test_split(\n",
    "    X, y_start, y_duration, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Combine targets\n",
    "y_train_combined = np.column_stack([y_start_train, y_duration_train])\n",
    "y_test_combined = np.column_stack([y_start_test, y_duration_test])\n",
    "\n",
    "# Create and train deep model\n",
    "print(\"üß† Training Deep Learning Model...\")\n",
    "deep_model = create_deep_model(X_train_scaled.shape[1])\n",
    "\n",
    "# Add callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=10, factor=0.5)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = deep_model.fit(\n",
    "    X_train_scaled, y_train_combined,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate deep model\n",
    "y_pred_combined = deep_model.predict(X_test_scaled)\n",
    "y_start_pred_deep = y_pred_combined[:, 0]\n",
    "y_duration_pred_deep = y_pred_combined[:, 1]\n",
    "\n",
    "deep_start_mae = mean_absolute_error(y_start_test, y_start_pred_deep)\n",
    "deep_start_r2 = r2_score(y_start_test, y_start_pred_deep)\n",
    "deep_duration_mae = mean_absolute_error(y_duration_test, y_duration_pred_deep)\n",
    "deep_duration_r2 = r2_score(y_duration_test, y_duration_pred_deep)\n",
    "\n",
    "print(f\"\\nüß† Deep Learning Results:\")\n",
    "print(f\"  Start Time - MAE: {deep_start_mae:.2f}h, R¬≤: {deep_start_r2:.3f}\")\n",
    "print(f\"  Duration - MAE: {deep_duration_mae:.2f}h, R¬≤: {deep_duration_r2:.3f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model and prepare for export\n",
    "all_results = results.copy()\n",
    "all_results['Deep Learning'] = {\n",
    "    'start_mae': deep_start_mae,\n",
    "    'start_r2': deep_start_r2,\n",
    "    'duration_mae': deep_duration_mae,\n",
    "    'duration_r2': deep_duration_r2\n",
    "}\n",
    "\n",
    "# Find best model based on combined performance\n",
    "model_scores = {}\n",
    "for name, metrics in all_results.items():\n",
    "    # Combined score: average R¬≤ (higher is better)\n",
    "    combined_r2 = (metrics['start_r2'] + metrics['duration_r2']) / 2\n",
    "    model_scores[name] = combined_r2\n",
    "\n",
    "best_model_name = max(model_scores, key=model_scores.get)\n",
    "print(f\"üèÜ Best Model: {best_model_name} (R¬≤ Score: {model_scores[best_model_name]:.3f})\")\n",
    "\n",
    "# Final results table\n",
    "final_results_df = pd.DataFrame(all_results).T\n",
    "final_results_df['combined_r2'] = final_results_df[['start_r2', 'duration_r2']].mean(axis=1)\n",
    "final_results_df = final_results_df.sort_values('combined_r2', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Final Model Rankings:\")\n",
    "print(final_results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13598ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the best model for deployment\n",
    "def export_model_for_deployment():\n",
    "    export_data = {\n",
    "        'model_type': best_model_name,\n",
    "        'feature_names': feature_names,\n",
    "        'performance': all_results[best_model_name],\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'data_points': len(featured_df)\n",
    "    }\n",
    "    \n",
    "    if best_model_name == 'Deep Learning':\n",
    "        # Save TensorFlow model\n",
    "        deep_model.save('sleep_prediction_model.h5')\n",
    "        \n",
    "        # Save scaler\n",
    "        joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "        \n",
    "        export_data['model_files'] = ['sleep_prediction_model.h5', 'feature_scaler.pkl']\n",
    "        export_data['requires_tensorflow'] = True\n",
    "        \n",
    "    else:\n",
    "        # Save scikit-learn models\n",
    "        model_data = trained_models[best_model_name]\n",
    "        joblib.dump(model_data['start_model'], 'start_time_model.pkl')\n",
    "        joblib.dump(model_data['duration_model'], 'duration_model.pkl')\n",
    "        \n",
    "        if model_data['scaler']:\n",
    "            joblib.dump(model_data['scaler'], 'feature_scaler.pkl')\n",
    "            export_data['model_files'] = ['start_time_model.pkl', 'duration_model.pkl', 'feature_scaler.pkl']\n",
    "        else:\n",
    "            export_data['model_files'] = ['start_time_model.pkl', 'duration_model.pkl']\n",
    "        \n",
    "        export_data['requires_tensorflow'] = False\n",
    "    \n",
    "    # Save model metadata\n",
    "    with open('model_config.json', 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Model exported successfully!\")\n",
    "    print(f\"üìÅ Files: {export_data['model_files']} + model_config.json\")\n",
    "    \n",
    "    return export_data\n",
    "\n",
    "# Export the model\n",
    "export_info = export_model_for_deployment()\n",
    "\n",
    "# Show export summary\n",
    "print(\"\\nüìã Export Summary:\")\n",
    "print(f\"Model Type: {export_info['model_type']}\")\n",
    "print(f\"Performance - Start Time R¬≤: {export_info['performance']['start_r2']:.3f}\")\n",
    "print(f\"Performance - Duration R¬≤: {export_info['performance']['duration_r2']:.3f}\")\n",
    "print(f\"Training Data Points: {export_info['data_points']}\")\n",
    "print(f\"Features Used: {len(export_info['feature_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all model files\n",
    "print(\"üì• Downloading model files...\")\n",
    "\n",
    "# Download model config\n",
    "files.download('model_config.json')\n",
    "\n",
    "# Download model files\n",
    "for file_name in export_info['model_files']:\n",
    "    if os.path.exists(file_name):\n",
    "        files.download(file_name)\n",
    "        print(f\"‚úÖ Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {file_name}\")\n",
    "\n",
    "print(\"\\nüéâ All files downloaded! Upload these to your Vercel project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab2cfc",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps for Vercel Deployment\n",
    "\n",
    "1. **Upload the downloaded files** to your Vercel project in a `models/` directory\n",
    "2. **Install required packages** in your `package.json`:\n",
    "   - For TensorFlow: `@tensorflow/tfjs-node`\n",
    "   - For scikit-learn models: `ml-matrix` or similar JS ML library\n",
    "3. **Update your prediction API** to use the trained model\n",
    "4. **Test the deployed model** with real predictions\n",
    "\n",
    "The trained model should provide much better accuracy than the simple linear regression!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
